<!DOCTYPE HTML>
<!--
	Editorial by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>AI Systems As State Actors - Morgan Klaus Scheuerman</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="/assets/css/main.css" />
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Main -->
					<div id="main">
						<div class="inner">

							<!-- Header -->
							<header id="header">
								<ul class="icons">
									<div class="header-left"><li>
										<p style="font-size:15px;"><a href="/readings.html"><span class="icon solid fa-arrow-left"> Back to Reading List</span></a></p>
									</li></div>
									<li><b>SOCIAL</b><img src="https://static.wixstatic.com/media/697bc8_3b4cf3d1a498494e8b6e097004ae72b1~mv2.gif" height="12px" width="12px"></li>
									<li><a href="https://twitter.com/morganklauss" class="icon brands fa-twitter"><span class="label">Twitter</span></a></li>
									<li><a href="https://medium.com/@morganklausscheuerman" class="icon brands fa-medium-m"><span class="label">Medium</span></a></li>
									<li><a href="https://www.linkedin.com/in/morgan-scheuerman/" class="icon brands fa-linkedin" aria-label="LinkedIn"></a></li>
									<li><a href="https://github.com/morganklauss" class="icon brands fa-github"><span class="label">Github</span></a></li>
								</ul>
							</header>

							<!-- Content -->
								<section>

									<h2>AI Systems As State Actors</h2>
									
									Kate Crawford and Jason Schultz, 2019 'AI Systems As State Actors', Columbia Law Review, 119(7), 1941-1972. https://columbialawreview.org/content/ai-systems-as-state-actors/
									<br><br>

									Few accountability frameworks have worked to hold AI systems accountable to fair, unbiased, due process. 
									Crawford and Schultz state this is because "they have
									failed to address the larger social and structural aspects of the problems
									or because there is a lack of political will to implement them" (pg. 1943). Such frameworks have also hardly been used to address vendors.
									There is an accountability gap concerning algorithmic systems that are used in government decisionmaking, where laws have not been applied to private companies supplying AI architectures and government 
									officials claim no knowledge or understanding of how the systems work. The authors argue for courts to address this gap by 
									applying state action doctrine to third party vendors, as third party vendors supplying AI for government decisionmaking should be 
									considered state actors for the purposes of constitutional liability. <br><br>

									<h3>Seeing Like a State AI System</h3>

									There is currently no comprehensive method for tracking the use of AI in government in the U.S., which also makes assessing their impact on the public 
									difficult. Some systems are developed entirely in-house by governments, while others are developed by contractors or as a donation. 
									They highlight two challenges in public accountability: "(1) lack of clear public accountability and oversight processes; and (2) objections from vendors that any
									real insights into their technology would reveal trade secrets or other
									confidential information" (pg. 1944). They point out some known government uses of AI, such as Palantir's supplying of AI systems for ICE and the Trump Administration's focus on algorithmic of disability fraud using social media data. <br><br>

									They discuss case studies where government algorithmic systems were taken to court, such as the home-care assessment systems in AK and IL in 2016 which were subsequently made illegal. They discuss how 
									algorithms are designed not with constitutional liability in mind, but cutting costs for populations considered "expensive," often marginalized populations. 
									The authors write: "an algorithmic system itself,
									optimized to cut costs without consideration of legal or policy concerns,
									created the core constitutional problems that ultimately decided the lawsuits" (pg. 1950). A lack of accountability and bias proliferation is further problematized as 
									systems become adopted from state to state through "software contractor migration," oftening training it on one's state population but applying it to another. <br><br>

									Litigation is primarily aimed at the government entities, but not the vendors. While accountability can often be enacted against government actors around specific problems, they do not prevent 
									future harms from AI systems. Justice is taken against government agencies, but has little effect on the design or implementation of systems. In one example of litigation against an AI system directly, the company which built the teacher evaluation system 
									fought to keep its algorithm secret from the plaintiffs' experts. In other cases, like the specific example of a protested criminal risk score, the judge was convinced to find the score inadmissible, but the system was 
									not barred from use.<br><br>

									<h3>A Framework for Private Actor Constitutional Acountability</h3>

									"Constitutional liability doctrines, including liability under 42 U.S.C. § 1983,
									have traditionally focused on the activities of public actors, such as
									government agencies or officials. These doctrines operate under the
									assumption that government actors have both the greatest power and
									responsibility for upholding those rights and protections, and should
									therefore be held to the highest levels of accountability Meanwhile,
									private actors, such as corporations or citizens, need only be held
									accountable under traditional tort or regulatory approaches" (pg. 19570). They state that courts have now been forced to 
									adapt their state action doctrine for private party liability, generally relying on three tests: "(1) the public function test,
									which asks whether the private entity performed a function traditionally
									and exclusively performed by government; (2) the compulsion test,
									which asks whether the state significantly encouraged or exercised coercive power over the private entity’s actions;101 and (3) the joint participation test, which asks whether the role of private actors was “pervasively
									entwined” with public institutions and officials" (pg. 1959). Yet there has been no model of consistency in cases for courts to follow. Crawford and Schultz examine the three tests to determine their applicability to AI vendors. 

									<br><br>
									<b>(1) The public function test:</b> whether the private company has performed a core governmental function that has been 
									exclusively/traditionally performed by the state.<br>
									Although few functions are solely performed by the state anymore, courts may still rule that the function is within the scope of the state rather than the private entity. 
									"When private AI vendors provide their software to governments to fulfill duties that are specifically tied to a state’s overall public
									and constitutional obligations, the possibility of the vendor being held a
									state actor becomes a reality" (pg. 1962). The question becomes about when AI is merely a tool for the government employee to 
									perform state functions or whether the system performs that function itself. If viewed as a tool, the vendor is outside the purview of 
									public functioning. <br><br>

									<b>(2) The compulsion theory:</b> "the extent to which the private entity
									has discretion to make substantive choices that impact constitutional concerns" (pg. 1964). <br>
									Who controls the design and implementation and data for a system is relevant to liability. If the state provides significant encouragement and direction in a system's implementation and maintenance, the state is likely to be seen 
									as constitutionally liable. <br><br>

									<b>(3) Joint participation theory:</b> "whether the government was
									significantly involved in the challenged action that is alleged to have
									caused the constitutional harm, so much so that the two entities can be
									considered joint participants ... If the government were
									merely involved through standard setting but not active decisionmaking,
									no joint participation exists" (pg. 1966). <br><br>

									<h3>What Courts Should Do</h3>

									Above, Crawford and Schultz discuss how courts could assess algorithmic vendor accountability. Next, they discuss what they 
									think courts when courts should be intervening. <br><br>
									
									<b>(1) When the state lacks sufficient accountability or capacity to provide appropriate remedies</b><br>
									When state accountability is weakest, like when the state relies on a private vendor for virtually all design and implementation 
									of an AI system, and the state lacks in capacity to address the consititutional harm caused. "The state had very little knowledge of how the AI software code
									had been written, where the mistakes were made, what data had been
									used to train and test it, or what means were required to mitigate the
									concerns raised in the case" (pg. 1969). They argue that holding a private vendor accountable, rather than simply expecting the state to 
									correct the vendor, particularly in cases of large institutional harm by a system, provides incentives for the vendors to mitigate harm. "Unless vendors are subject to the court’s
									jurisdiction, the court cannot assert any real oversight or impose any specific injunctive relief on that party, even if it is in the best position to fix
									errors in how the AI performed" (pg. 1970). <br><br>

									<b>(2) When AI providers are unregulated</b><br>
									There is very little regulation in place for AI vendor accountability. They argue state action remedies could set norms for viewing 
									AI vendors as state actors, while expecting harmed plaintiffs to individually sue vendors would risk viewing them as not acting 
									on behalf of the state. <br><br>

									<b>(3)When trade secrecy or third-Party technical information is at the heart of
										the constitutional liability question </b><br>
										In the case where the vendor strives for opacity, state action should intervene. Otherwise, government employees may be unable to provide any 
										answers about how the system functions. "In such cases,
										considering the vendor a state actor would allow courts access to the
										necessary information to decide cases while also directly addressing vendor trade secrecy concerns" (pg. 1971). 


				



								
																		
									
									
								</section>

						</div>
					</div>

				<!-- Sidebar -->
				<div id="sidebar">
					<div class="inner">

						<!-- Menu -->
						<section>
								<header class="main">
									<center><a href="#" class="image"><img src="/images/mainphoto.jpg" alt="" width="200" height="200"/></a>								
									<h1>Morgan Klaus Scheuerman</h1>
								<article>
									<p>PhD Student in <i>Information Science</i> at
									University of Colorado Boulder
									</p>
								</center>
								</article>
								</header>
								<header class="major">
						</section>
						<nav id="menu">
								<ul>
									<li><a href="/index.html">Home</a></li>
									<li><a href="/pdfs/misc/ScheuermanCV.pdf">CV</a></li>
									<li><a href="mailto:morgan.scheuerman@colorado.edu">Contact</a></li>
									<li><a href="/research.html">Research</a></li>
									<li><a href="/news.html">News</a></li>
									<li><a href="/blog.html">Blog</a></li>
									<li>
										<span class="opener">Projects</span>
										<ul>
											<li><a href="/gender-guidelines.html">HCI Gender Guidelines</a></li>
											<li><a href="/readings.html">Reading List</a></li>
										</ul>
									</li>
							</nav>
						<!--Section-->
						<section>
							<center><a href="/gender-guidelines.html" class="button big">HCI Gender Guidelines</a></center>

							<br>
						<!-- Footer -->
							<footer id="footer">
								<p class="copyright">&copy; Morgan Klaus Scheuerman. 2020. All rights reserved.
									Design adpted from <a href="https://html5up.net">HTML5 UP</a>.</p>
							</footer>

					</div>
					</div>

			</div>

		<!-- Scripts -->
			<script src="/assets/js/jquery.min.js"></script>
			<script src="/assets/js/browser.min.js"></script>
			<script src="/assets/js/breakpoints.min.js"></script>
			<script src="/assets/js/util.js"></script>
			<script src="/assets/js/main.js"></script>

	</body>
</html>